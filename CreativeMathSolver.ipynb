{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers accelerate peft datasets bitsandbytes torch\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\nfrom peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nimport bitsandbytes as bnb\nimport torch.optim as optim\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import load_dataset, Dataset\nfrom torch.utils.data import DataLoader\n\nMODEL_NAME = \"deepseek-ai/deepseek-math-7b-base\"\nSAVE_PATH = \"finetuned_deepseek_math\"\nMAX_LENGTH = 128\nBATCH_SIZE = 1\nNUM_EPOCHS = 3\n\n# 4-bit quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",            # Set the quantization type (nf4 is a common choice)\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load tokenizer and model in 4-bit mode\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    quantization_config=bnb_config\n)\nmodel.generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\n# Prepare model for k-bit training and wrap with LoRA via PEFT\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(\n    r=20,\n    lora_alpha=40,\n    target_modules=[\"q_proj\", \"v_proj\"],  # Adjust these target modules as needed for your model architecture\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\n\n# Load CSV dataset using the datasets library.\n# Your CSV is assumed to have columns: \"problem\" and \"solution\"\nraw_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/dataset-math/dataset2.csv\")[\"train\"]\n\n# Preprocessing: Create a text field combining the problem and solution.\ndef preprocess(example):\n    example[\"text\"] = f\"Problem Statement: {example['problem']}\\nSolution: {example['solution']}\"\n    return example\n\nprocessed_dataset = raw_dataset.map(preprocess)\n\n# Tokenize the prompts\ndef tokenize_fn(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n    \ntokenized_dataset = processed_dataset.map(tokenize_fn, batched=True)\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n\n# Create DataLoader\ntrain_dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:27:56.572279Z","iopub.execute_input":"2025-03-13T18:27:56.572553Z","iopub.status.idle":"2025-03-13T18:29:54.088338Z","shell.execute_reply.started":"2025-03-13T18:27:56.572530Z","shell.execute_reply":"2025-03-13T18:29:54.087337Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae4a0643285476cb8b6c25d3de810c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c27c86607f4801a6086ec4ecc31536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5800c53ae6774824bdaf5aca47339818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996134b2712a4ab1ae09d03685a18b10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462011a7ae244f43bfb1cccc3b365bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806bbfb4fd4a4b859801147a0d8b68ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46bd6901187042f9bb1a87e9795c4bdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd563ebcfbff4f6abb6eabaefa9ec491"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4752e2b38456431ea28ec6d0182d0d78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30059f7ba1b94dd4be984aa3aa735935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3928c50ee1e0460c8dca26f6273fb724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc1383d9c2e0458c928e940610f18726"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Raw finetuning loop\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 10\nmodel.train()\nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attention_mask = batch[\"attention_mask\"].to(model.device)\n        # Use input_ids as labels for causal LM training.\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss / len(train_dataloader):.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:36:27.742887Z","iopub.execute_input":"2025-03-13T18:36:27.743176Z","iopub.status.idle":"2025-03-13T18:43:53.788918Z","shell.execute_reply.started":"2025-03-13T18:36:27.743153Z","shell.execute_reply":"2025-03-13T18:43:53.788174Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 Loss: 17.4189\nEpoch 2/10 Loss: 4.9337\nEpoch 3/10 Loss: 2.0824\nEpoch 4/10 Loss: 1.0375\nEpoch 5/10 Loss: 0.7017\nEpoch 6/10 Loss: 0.5019\nEpoch 7/10 Loss: 0.3605\nEpoch 8/10 Loss: 0.2809\nEpoch 9/10 Loss: 0.2426\nEpoch 10/10 Loss: 0.2252\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Save the fine-tuned model (PEFT adapter weights will be saved)\nsave_path = \"finetuned_deepseek_math\"\nmodel.save_pretrained(save_path)\nprint(f\"Fine-tuned model saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:46:24.337572Z","iopub.execute_input":"2025-03-13T18:46:24.337936Z","iopub.status.idle":"2025-03-13T18:46:24.752316Z","shell.execute_reply.started":"2025-03-13T18:46:24.337904Z","shell.execute_reply":"2025-03-13T18:46:24.751593Z"}},"outputs":[{"name":"stdout","text":"Fine-tuned model saved to finetuned_deepseek_math\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nSAVE_PATH = \"finetuned_deepseek_math\"\nmodel_name = \"deepseek-ai/deepseek-math-7b-base\"\n\n# Load tokenizer and base model with quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    quantization_config=bnb_config\n)\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\n# Prepare model for k-bit training and build LoRA configuration (should match training settings)\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(\n    r=20,\n    lora_alpha=40,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\n\n# Load the fine-tuned adapter weights\nmodel.load_pretrained(SAVE_PATH)\nprint(f\"Loaded fine-tuned model from {SAVE_PATH}\")\n\n# Testing on new emoji problems using the loaded, fine-tuned model\nmodel.eval()\ntest_prompts = [\n    \"üöó + üöó + üöó + üöó = 20 ‚Üí üöó =\",\n    \"üéà + üéà + üéà = 15 ‚Üí üéà =\",\n    \"üê∂ + üê∂ = 12 ‚Üí üê∂ =\"\n]\n\nprint(\"\\nTest Results:\")\nfor prompt in test_prompts:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_new_tokens=20,\n            generation_config=model.generation_config\n        )\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"\\nInput: {prompt}\\nOutput: {result}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing on new emoji problems using the loaded, fine-tuned model\nmodel.eval()\ntest_prompts = [\n    \"üöó + üöó + üöó + üöó = 20 ‚Üí üöó =\",\n    \"üéà + üéà + üéà = 15 ‚Üí üéà =\",\n    \"üê∂ + üê∂ = 12 ‚Üí üê∂ =\"\n]\n\nprint(\"\\nTest Results:\")\nfor prompt in test_prompts:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_new_tokens=20,\n            generation_config=model.generation_config\n        )\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"\\nInput: {prompt}\\nOutput: {result}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:47:16.682425Z","iopub.execute_input":"2025-03-13T18:47:16.682776Z","iopub.status.idle":"2025-03-13T18:47:18.224982Z","shell.execute_reply.started":"2025-03-13T18:47:16.682743Z","shell.execute_reply":"2025-03-13T18:47:18.224217Z"}},"outputs":[{"name":"stdout","text":"\nTest Results:\n\nInput: üöó + üöó + üöó + üöó = 20 ‚Üí üöó =\nOutput: üöó + üöó + üöó + üöó = 20 ‚Üí üöó = 4\n\n\nInput: üéà + üéà + üéà = 15 ‚Üí üéà =\nOutput: üéà + üéà + üéà = 15 ‚Üí üéà = 5\n\n\nInput: üê∂ + üê∂ = 12 ‚Üí üê∂ =\nOutput: üê∂ + üê∂ = 12 ‚Üí üê∂ = 6\n\n","output_type":"stream"}],"execution_count":7}]}